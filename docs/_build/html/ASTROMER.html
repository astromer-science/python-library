<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ASTROMER &mdash; ASTROMER 0.0.6 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to ASTROMER documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #cf3a23" >

          
          
          <a href="index.html" class="icon icon-home">
            ASTROMER
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">LIBRARY DOCUMENTATION</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">ASTROMER</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#single-band-encoder">Single-Band Encoder</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ASTROMER.models.SingleBandEncoder"><code class="docutils literal notranslate"><span class="pre">SingleBandEncoder</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ASTROMER.models.SingleBandEncoder.encode"><code class="docutils literal notranslate"><span class="pre">SingleBandEncoder.encode()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ASTROMER.models.SingleBandEncoder.fit"><code class="docutils literal notranslate"><span class="pre">SingleBandEncoder.fit()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ASTROMER.models.SingleBandEncoder.from_pretraining"><code class="docutils literal notranslate"><span class="pre">SingleBandEncoder.from_pretraining()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ASTROMER.models.SingleBandEncoder.load_weights"><code class="docutils literal notranslate"><span class="pre">SingleBandEncoder.load_weights()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#preprocessing">Preprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ASTROMER.preprocessing.make_pretraining"><code class="docutils literal notranslate"><span class="pre">make_pretraining()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-ASTROMER.utils">Utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ASTROMER.utils.download_weights"><code class="docutils literal notranslate"><span class="pre">download_weights()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick-start</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fine-tune">Fine Tune</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #cf3a23" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ASTROMER</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ASTROMER</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/ASTROMER.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="astromer">
<h1>ASTROMER<a class="headerlink" href="#astromer" title="Permalink to this heading"></a></h1>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this heading"></a></h2>
</section>
<section id="single-band-encoder">
<h2>Single-Band Encoder<a class="headerlink" href="#single-band-encoder" title="Permalink to this heading"></a></h2>
<blockquote class="epigraph">
<div><p>The <strong>Single-Band Encoder</strong> represents the main class of the models, which load, fit, encode and train the preprocessed weights.</p>
</div></blockquote>
<blockquote class="epigraph">
<div><p>It took every single-band light curve that may vary between different stars, and this depends on the objectives of the survey being carried out.</p>
</div></blockquote>
<blockquote class="epigraph">
<div><p>The X is a set of observations of a celestial object over time (such as a star). Each observation had two characteristics: the magnitude (brightness) of the object and the Modified Julian Date (MJD) when the observation was made.</p>
</div></blockquote>
<blockquote class="epigraph">
<div><p>We propose to use learned representations of a transformer-based encoder to create embeddings that represent the variability of objects in dk.dimensional space. Making easy to fine-tune the model weights to match other surveys and use them to solve downstream task, such as classification or regression.</p>
</div></blockquote>
<span class="target" id="module-ASTROMER.models"></span><dl class="py class">
<dt class="sig sig-object py" id="ASTROMER.models.SingleBandEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ASTROMER.models.</span></span><span class="sig-name descname"><span class="pre">SingleBandEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxlen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/models.html#SingleBandEncoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.models.SingleBandEncoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class is a transformer-based model that process the input and generate a fixed-size representation
Since each light curve has two characteristics (magnitude and time) we transform into
embeddings Z = 200x256.</p>
<p>The maximum number of observations remain fixed and masked, so every Z had the same length even if some
light curves are shorter than others.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_layer</strong> (<em>Integer</em>) – Number of self-attention blocks or transformer layers in the encoder.</p></li>
<li><p><strong>d_model</strong> (<em>Integer</em>) – Determines the dimensionality of the model’s internal representation (must be divisible by ‘num_heads’).</p></li>
<li><p><strong>num_heads</strong> (<em>Integer</em>) – Number of attention heads used in an attention layer.</p></li>
<li><p><strong>dff</strong> (<em>Integer</em>) – Number of neurons for the fully-connected layer applied after the attention layers. It consists of two linear transformations with a non-linear activation function in between.</p></li>
<li><p><strong>base</strong> (<em>Float32</em>) – Value that defines the maximum and minimum wavelengths of the positional encoder (see equation 4 on Oliva-Donoso et al. 2022). Is used to define the range of positions the attention mechanism uses to compute the attention weights.</p></li>
<li><p><strong>dropout</strong> (<em>Float32</em>) – Regularization applied to output of the fully-connected layer to prevent overfitting. Randomly dropping out (i.e., setting to zero) some fraction of the input units in a layer during training.</p></li>
<li><p><strong>maxlen</strong> (<em>Integer</em>) – Maximum length to process in the encoder. It is used in the SingleBandEncoder class to limit the input sequences’ length when passed to the transformer-based model.</p></li>
<li><p><strong>batch_size</strong> (<em>Integer</em>) – Number of samples to be used in a forward pass. Note an epoch is completed when all batches were processed (default none).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ASTROMER.models.SingleBandEncoder.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oids_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concatenate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/models.html#SingleBandEncoder.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.models.SingleBandEncoder.encode" title="Permalink to this definition"></a></dt>
<dd><p>This method encodes a dataset of light curves into a fixed-dimensional embedding using the ASTROMER encoder.
The method first checks the format of the dataset containing the light curves.</p>
<p>Then, it loads the dataset using predefined functions from the ‘data’ module. In this part, if a light curve contains more than 200 observations, ASTROMER will divide it into shorter windows of 200 length.</p>
<p>After loading data, the data pass through the encoder layer to obtain the embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – The input data to be encoded. It can be a list of numpy arrays or a tensorflow dataset.</p></li>
<li><p><strong>oids_list</strong> (<em>List</em>) – list of object IDs. Since ASTROMER can only process fixed sequence of 200 observations, providing the IDs allows the model to concatenate windows when the length of the objects is larger than 200.</p></li>
<li><p><strong>labels</strong> – an optional list of labels for the objects associated to the input dataset.</p></li>
<li><p><strong>batch_size</strong> – the number of samples to be used in a forward-pass within the encoder. Default is 1.</p></li>
<li><p><strong>concatenate</strong> (<em>Boolean</em>) – a boolean indicating whether to concatenate the embeddings of objects with the same ID into a single vector.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ASTROMER.models.SingleBandEncoder.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_batches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">project_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/models.html#SingleBandEncoder.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.models.SingleBandEncoder.fit" title="Permalink to this definition"></a></dt>
<dd><p>The ‘fit()’ method trains ASTROMER for a given number of epochs. After each epoch, the model’s performance is evaluated on the validation data, and the training stops if there is no improvement in a specified number of epochs (patience).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_batches</strong> (<em>Object</em>) – Training data already formatted as TF.data.Dataset</p></li>
<li><p><strong>valid_batches</strong> (<em>Object</em>) – Validation data already formatted as TF.data.Dataset</p></li>
<li><p><strong>epochs</strong> (<em>Integer</em>) – Number of training loops in where all light curves have been processed.</p></li>
<li><p><strong>patience</strong> (<em>Integer</em>) – The number of epochs with no improvement after which training will be stopped.</p></li>
<li><p><strong>lr</strong> (<em>Float32</em>) – A float specifying the learning rate</p></li>
<li><p><strong>project_path</strong> – Path for saving weights and training logs</p></li>
<li><p><strong>verbose</strong> (<em>Integer</em>) – if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.”</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ASTROMER.models.SingleBandEncoder.from_pretraining">
<span class="sig-name descname"><span class="pre">from_pretraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'macho'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/models.html#SingleBandEncoder.from_pretraining"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.models.SingleBandEncoder.from_pretraining" title="Permalink to this definition"></a></dt>
<dd><p>Loads a pre-trained model with pre-trained weights for a specific astronomical dataset. This method allows users to easily load pre-trained models for astronomical time-series datasets and use them for their purposes.</p>
<p>This method checks if you have the weights locally, if not then downloads and then uploads them.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> – Corresponds to the name of the survey used to pre-train ASTROMER. The name of the survey should match with the name of the zip file in <a class="reference external" href="https://github.com/astromer-science/weights">https://github.com/astromer-science/weights</a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ASTROMER.models.SingleBandEncoder.load_weights">
<span class="sig-name descname"><span class="pre">load_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights_folder</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/models.html#SingleBandEncoder.load_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.models.SingleBandEncoder.load_weights" title="Permalink to this definition"></a></dt>
<dd><p>The ‘load_weights()’ method loads pre-trained parameters into the model architecture. The method loads the weights from the file located at {weights_folder}/weights directory, which is assumed to be in TensorFlow checkpoint format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>weights_folder</strong> – the path to the folder containing the pre-trained weights.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="preprocessing">
<h2>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this heading"></a></h2>
<span class="target" id="module-ASTROMER.preprocessing"></span><dl class="py function">
<dt class="sig sig-object py" id="ASTROMER.preprocessing.make_pretraining">
<span class="sig-prename descclassname"><span class="pre">ASTROMER.preprocessing.</span></span><span class="sig-name descname"><span class="pre">make_pretraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_obs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">msk_frac</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnd_frac</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">same_frac</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">numpy_args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/preprocessing.html#make_pretraining"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.preprocessing.make_pretraining" title="Permalink to this definition"></a></dt>
<dd><p>Load and format data to feed ASTROMER model. It can process either a list of bumpy arrays or tf.records. At the the end of this method, a tensorflow dataset is generated following the preprocessing pipeline explained in Section 5.3 (Donoso-Oliva, et al. 2022)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>object</em>) – The data set containing the light curves.</p></li>
<li><p><strong>batch_size</strong> (<em>Integer</em>) – This integer determines the number of subsets that we will pass to our model.</p></li>
<li><p><strong>shuffle</strong> (<em>Boolean</em>) – A boolean indicating whether to rearrange samples randomly</p></li>
<li><p><strong>sampling</strong> (<em>Boolean</em>) – A Boolean that when is true, indicates the model to take samples of every light curve instead of all observation samples.</p></li>
<li><p><strong>max_obs</strong> (<em>Integer</em>) – This Integer indicates how big each lightcurve sample will be. e.g. (with max_obs = 100): The length of a light curve is 720 observations so the model will generate 7 blocks of 100 observations, and the sample with 20 cases will be completed using padding with zero values after the last point in order to obtain a sequence of length 100.</p></li>
<li><p><strong>msk_frac</strong> (<em>Float32</em>) – The fraction of samples that will be masked by the model</p></li>
<li><p><strong>rnd_frac</strong> (<em>Float32</em>) – The fraction of samples in which their values will be changed by random numbers.</p></li>
<li><p><strong>same_frac</strong> (<em>Float32</em>) – It is the fraction of the masked observations that you unmask and allow to be processed in the attention layer</p></li>
<li><p><strong>repeat</strong> (<em>Integer</em>) – This Integer determines the number of times the same data set is repeated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-ASTROMER.utils">
<span id="utils"></span><h2>Utils<a class="headerlink" href="#module-ASTROMER.utils" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="ASTROMER.utils.download_weights">
<span class="sig-prename descclassname"><span class="pre">ASTROMER.utils.</span></span><span class="sig-name descname"><span class="pre">download_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ASTROMER/utils.html#download_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ASTROMER.utils.download_weights" title="Permalink to this definition"></a></dt>
<dd><p>This method delivers the weights requested in the SingleBandEncoder class using the method ‘from_pretraining()’ that specifies the available surveys; ‘macho’, ‘atlas’ and ‘ztfg’.
The UTILS module it’s a set of functions that allow performing functions not considered in models and preprocessing.</p>
<p>This code provides a simple and convenient way to download and extract zipped files from a URL to a specified directory using Python.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> – </p></li>
<li><p><strong>target</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="quick-start">
<h2>Quick-start<a class="headerlink" href="#quick-start" title="Permalink to this heading"></a></h2>
<blockquote class="epigraph">
<div><p><strong>Install</strong></p>
</div></blockquote>
<blockquote class="epigraph">
<div><p>First, install the ASTROMER wheel using pip</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">ASTROMER</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ASTROMER.models</span> <span class="kn">import</span> <span class="n">SingleBandEncoder</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>Then initiate</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleBandEncoder</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">from_pretraining</span><span class="p">(</span><span class="s1">&#39;macho&#39;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>It will automatically download the weights from this public github repository and load them into the SingleBandEncoder instance. Assuming you have a list of vary-lenght (numpy) light curves.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">samples_collection</span> <span class="o">=</span> <span class="p">[</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5200</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">5300</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">5400</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>Light curves are Lx3 matrices with time, magnitude, and magnitude std. To encode samples use:</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attention_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">samples_collection</span><span class="p">,</span>
                                  <span class="n">oids_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">],</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">concatenate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<section id="fine-tune">
<h3>Fine Tune<a class="headerlink" href="#fine-tune" title="Permalink to this heading"></a></h3>
<blockquote class="epigraph">
<div><p><cite>ASTROMER</cite> can be easly trained by using the <cite>fit</cite>. It include</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ASTROMER</span> <span class="kn">import</span> <span class="n">SingleBandEncoder</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SingleBandEncoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                       <span class="n">d_model</span>   <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
                       <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                       <span class="n">dff</span>       <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
                       <span class="n">base</span>      <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                       <span class="n">dropout</span>   <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">maxlen</span>    <span class="o">=</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;macho&#39;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>where,</p>
</div></blockquote>
<ul class="simple">
<li><p><cite>num_layers</cite>: Number of self-attention blocks</p></li>
<li><p><cite>d_model</cite>: Self-attention block dimension (must be divisible by <cite>num_heads</cite>)</p></li>
<li><p><cite>num_heads</cite>: Number of heads within the self-attention block</p></li>
<li><p><cite>dff</cite>: Number of neurons for the fully-connected layer applied after the attention blocks</p></li>
<li><p><cite>base</cite>: Positional encoder base (see formula)</p></li>
<li><p><cite>dropout</cite>: Dropout applied to output of the fully-connected layer</p></li>
<li><p><cite>maxlen</cite>: Maximum length to process in the encoder</p></li>
</ul>
<blockquote class="epigraph">
<div><p>Notice you can ignore <cite>model.from_pretrained(‘macho’)</cite> for clean training.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mode</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
      <span class="n">validation_data</span><span class="p">,</span>
      <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
      <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
      <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
      <span class="n">project_path</span><span class="o">=</span><span class="s1">&#39;./my_folder&#39;</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<blockquote class="epigraph">
<div><p>where,</p>
</div></blockquote>
<ul class="simple">
<li><p><cite>train_data</cite>: Training data already formatted as tf.data</p></li>
<li><p><cite>validation_data</cite>: Validation data already formatted as tf.data</p></li>
<li><p><cite>epochs</cite>: Number of epochs for training</p></li>
<li><p><cite>patience</cite>: Early stopping patience</p></li>
<li><p><cite>lr</cite>: Learning rate</p></li>
<li><p><cite>project_path</cite>: Path for saving weights and training logs</p></li>
<li><p><cite>verbose</cite>: (0) Display information during training (1) don’t</p></li>
</ul>
<p><cite>train_data</cite> and <cite>validation_data</cite> should be loaded using <cite>load_numpy</cite> or <cite>pretraining_records</cite> functions. Both functions are in the <cite>ASTROMER.preprocessing</cite> module.</p>
<blockquote class="epigraph">
<div><p>For large datasets is recommended to use Tensorflow Records <a class="reference external" href="https://github.com/astromer-science/main-code/blob/main/presentation/notebooks/create_records.ipynb">see this tutorial to execute our data pipeline</a></p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to ASTROMER documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Simon Salazar.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>